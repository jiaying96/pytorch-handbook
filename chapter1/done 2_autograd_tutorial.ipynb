{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Autograd: 自动求导机制\n",
    "===================================\n",
    "\n",
    "PyTorch 中所有神经网络的核心是 ``autograd`` 包。\n",
    "我们先简单介绍一下这个包，然后训练第一个简单的神经网络。\n",
    "\n",
    "``autograd``包为张量上的所有操作提供了自动求导。\n",
    "它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。\n",
    "\n",
    "\n",
    "示例\n",
    "\n",
    "张量（Tensor）\n",
    "--------\n",
    "\n",
    "``torch.Tensor``是这个包的核心类。如果设置\n",
    "``.requires_grad`` 为 ``True``，那么将会追踪所有对于该张量的操作。 \n",
    "当完成计算后通过调用 ``.backward()``，自动计算所有的梯度，\n",
    "这个张量的所有梯度将会自动积累到 ``.grad`` 属性。\n",
    "\n",
    "要阻止张量跟踪历史记录，可以调用``.detach()``方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。\n",
    "\n",
    "为了防止跟踪历史记录（和使用内存），可以将代码块包装在``with torch.no_grad()：``中。\n",
    "在评估模型时特别有用，因为模型可能具有`requires_grad = True`的可训练参数，但是我们不需要梯度计算。\n",
    "\n",
    "在自动梯度计算中还有另外一个重要的类``Function``.\n",
    "\n",
    "``Tensor`` and ``Function`` are interconnected and build up an acyclic\n",
    "graph, that encodes a complete history of computation. Each tensor has\n",
    "a ``.grad_fn`` attribute that references a ``Function`` that has created\n",
    "the ``Tensor`` (except for Tensors created by the user - their\n",
    "``grad_fn is None``).\n",
    "\n",
    "``Tensor`` 和 ``Function``互相连接并生成一个非循环图，它表示和存储了完整的计算历史。\n",
    "每个张量都有一个``.grad_fn``属性，这个属性引用了一个创建了``Tensor``的``Function``（除非这个张量是用户手动创建的，即，这个张量的\n",
    "``grad_fn`` 是 ``None``）。\n",
    "\n",
    "如果需要计算导数，你可以在``Tensor``上调用``.backward()``。 \n",
    "如果``Tensor``是一个标量（即它包含一个元素数据）则不需要为``backward()``指定任何参数，\n",
    "但是如果它有更多的元素，你需要指定一个``gradient`` 参数来匹配张量的形状。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***译者注：在其他的文章中你可能会看到说将Tensor包裹到Variable中提供自动梯度计算，Variable 这个在0.41版中已经被标注为过期了，现在可以直接使用Tensor，官方文档在这里：***\n",
    "(https://pytorch.org/docs/stable/autograd.html#variable-deprecated) \n",
    "\n",
    "具体的后面会有详细说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#创建一个张量并设置 requires_grad=True 用来追踪他的计算历史\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#对张量进行操作:\n",
    "y = x + 2\n",
    "print(y)\n",
    "#结果y已经被计算出来了，所以，grad_fn已经被自动生成了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x0000026FC4745F28>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "#对y进行一个操作\n",
    "z = y * y * 3\n",
    "\n",
    "out = z.mean()\n",
    "\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "a\n",
      "tensor([[ 2.0697, -0.5628],\n",
      "        [ 6.5453,  0.5896]], requires_grad=True)\n",
      "tensor([[ 4.2835,  0.3167],\n",
      "        [42.8409,  0.3476]], grad_fn=<MulBackward0>)\n",
      "b\n",
      "tensor(47.7887, grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x0000026FC2A56C50>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "#.requires_grad_( ... ) 可以改变现有张量的 requires_grad属性。\n",
    "# 如果没有指定的话，默认输入的flag是 False。\n",
    "print(a.requires_grad)  # False 默认\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad) # True\n",
    "print(\"a\")\n",
    "print(a)\n",
    "print((a * a))\n",
    "b = (a * a).sum()  # a中元素的平方和\n",
    "print(\"b\")\n",
    "print(b)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度\n",
    "---------\n",
    "反向传播\n",
    "因为 ``out``是一个纯量（scalar），``out.backward()`` 等于``out.backward(torch.tensor(1))``。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#反向传播 因为 out是一个纯量（scalar），out.backward() 等于out.backward(torch.tensor(1))。\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print gradients d(out)/dx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)  # d(out)/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到矩阵 ``4.5``.调用 ``out``\n",
    "*Tensor* “$o$”.\n",
    "\n",
    "x=tensor([[1., 1.],\n",
    "        [1., 1.]], requires_grad=True)\n",
    "        \n",
    "y=x+2   \n",
    "\n",
    "z=y * y * 3\n",
    "\n",
    "out=z.mean()\n",
    "\n",
    "得到 $o = \\frac{1}{4}\\sum_i z_i$,\n",
    "$z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$.\n",
    "\n",
    "因此,\n",
    "$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, hence\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用 autograd 做更多的操作\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "tensor([ 1.4972, -0.9856, -0.3498], requires_grad=True)\n",
      "y\n",
      "tensor([ 2.9944, -1.9712, -0.6996], grad_fn=<MulBackward0>)\n",
      "y.data.norm()\n",
      "tensor(3.6526)\n",
      "y\n",
      "tensor([ 5.9888, -3.9423, -1.3992], grad_fn=<MulBackward0>)\n",
      "y/x\n",
      "tensor([4., 4., 4.], grad_fn=<DivBackward0>)\n",
      "y.data.norm()\n",
      "tensor(7.3052)\n",
      "y\n",
      "tensor([11.9776, -7.8846, -2.7985], grad_fn=<MulBackward0>)\n",
      "y/x\n",
      "tensor([8., 8., 8.], grad_fn=<DivBackward0>)\n",
      "y.data.norm()\n",
      "tensor(14.6103)\n",
      "y\n",
      "tensor([ 23.9552, -15.7692,  -5.5969], grad_fn=<MulBackward0>)\n",
      "y/x\n",
      "tensor([16., 16., 16.], grad_fn=<DivBackward0>)\n",
      "y.data.norm()\n",
      "tensor(29.2206)\n",
      "y\n",
      "tensor([ 47.9104, -31.5384, -11.1938], grad_fn=<MulBackward0>)\n",
      "y/x\n",
      "tensor([32., 32., 32.], grad_fn=<DivBackward0>)\n",
      "y.data.norm()\n",
      "tensor(58.4412)\n",
      "y\n",
      "tensor([ 95.8208, -63.0768, -22.3876], grad_fn=<MulBackward0>)\n",
      "y/x\n",
      "tensor([64., 64., 64.], grad_fn=<DivBackward0>)\n",
      "y.data.norm()\n",
      "tensor(116.8825)\n",
      "y\n",
      "tensor([ 191.6415, -126.1537,  -44.7753], grad_fn=<MulBackward0>)\n",
      "y/x\n",
      "tensor([128., 128., 128.], grad_fn=<DivBackward0>)\n",
      "y.data.norm()\n",
      "tensor(233.7650)\n",
      "y\n",
      "tensor([ 383.2831, -252.3074,  -89.5506], grad_fn=<MulBackward0>)\n",
      "y/x\n",
      "tensor([256., 256., 256.], grad_fn=<DivBackward0>)\n",
      "y.data.norm()\n",
      "tensor(467.5299)\n",
      "y\n",
      "tensor([ 766.5661, -504.6147, -179.1012], grad_fn=<MulBackward0>)\n",
      "y/x\n",
      "tensor([512., 512., 512.], grad_fn=<DivBackward0>)\n",
      "y.data.norm()\n",
      "tensor(935.0598)\n",
      "y\n",
      "tensor([ 1533.1322, -1009.2295,  -358.2023], grad_fn=<MulBackward0>)\n",
      "y/x\n",
      "tensor([1024., 1024., 1024.], grad_fn=<DivBackward0>)\n",
      "y final\n",
      "tensor([ 1533.1322, -1009.2295,  -358.2023], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(\"x\")\n",
    "print(x)\n",
    "\n",
    "y = x * 2\n",
    "print(\"y\")\n",
    "print(y)\n",
    "while y.data.norm() < 1000:\n",
    "    print(\"y.data.norm()\")\n",
    "    print(y.data.norm())\n",
    "    y = y * 2\n",
    "    print(\"y\")\n",
    "    print(y)\n",
    "    print(\"y/x\")\n",
    "    print(y/x)\n",
    "print(\"y final\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "\n",
    "print(gradients.size())\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)\n",
    "'''  \n",
    "torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n",
    "分析：\n",
    "中间1.0  正常算出来的梯度 \n",
    "前面0.1  正常算出来的梯度 *0.1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果``.requires_grad=True``但是你又不希望进行autograd的计算，\n",
    "那么可以将变量包裹在 ``with torch.no_grad()``中:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4972, -0.9856, -0.3498], requires_grad=True)\n",
      "True\n",
      "tensor([2.2416, 0.9714, 0.1224], grad_fn=<PowBackward0>)\n",
      "tensor([2.2416, 0.9714, 0.1224], grad_fn=<MulBackward0>)\n",
      "tensor([ 2.9944, -1.9712, -0.6996], grad_fn=<MulBackward0>)\n",
      "True\n",
      "tensor([ 3.3561, -0.9573, -0.0428], grad_fn=<PowBackward0>)\n",
      "tensor([ 3.3561, -0.9573, -0.0428], grad_fn=<MulBackward0>)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.requires_grad)\n",
    "print((x ** 2))  # x的平方  tensor([2.2416, 0.9714, 0.1224], grad_fn=<PowBackward0>)\n",
    "print((x * x))  # x的平方 tensor([2.2416, 0.9714, 0.1224], grad_fn=<MulBackward0>)\n",
    "print((x * 2))  # x*2 tensor([ 2.9944, -1.9712, -0.6996], grad_fn=<MulBackward0>)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "\n",
    "print((x ** 3))  # x的3次方  tensor([2.2416, 0.9714, 0.1224], grad_fn=<PowBackward0>)\n",
    "print((x * x *x))  # x的3次方 tensor([2.2416, 0.9714, 0.1224], grad_fn=<MulBackward0>)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\tprint((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**稍后阅读:**\n",
    "\n",
    " ``autograd`` 和 ``Function`` 的官方文档 https://pytorch.org/docs/autograd\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch_exericise"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
